# -*- coding: utf-8 -*-
"""Mobilenetv2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cyrlPaQtIf7YSXzmp2sQC4oSdydxLta-
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=False)

import os, json, time, numpy as np, tensorflow as tf
from pathlib import Path
import pandas as pd

BASE_DIR = "/content/drive/MyDrive/Colab Notebooks"
ALL_DIR  = f"{BASE_DIR}/all"
TEST_DIR = f"{BASE_DIR}/TEST MIXED"

SEED, IMG_SIZE, BATCH_SIZE, VAL_SPLIT = 42, (224,224), 16, 0.15

raw_train_ds = tf.keras.utils.image_dataset_from_directory(
    ALL_DIR, validation_split=VAL_SPLIT, subset="training", seed=SEED,
    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=True
)

raw_val_ds = tf.keras.utils.image_dataset_from_directory(
    ALL_DIR, validation_split=VAL_SPLIT, subset="validation", seed=SEED,
    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False
)

raw_test_ds = tf.keras.utils.image_dataset_from_directory(
    TEST_DIR, image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False
)

class_names = raw_train_ds.class_names
num_classes = len(class_names)
print("âœ… Classes:", class_names)

train_ds = raw_train_ds.prefetch(tf.data.AUTOTUNE)
val_ds   = raw_val_ds.prefetch(tf.data.AUTOTUNE)
test_ds  = raw_test_ds.prefetch(tf.data.AUTOTUNE)

assert raw_test_ds.class_names == class_names, "âŒ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª ÙÙŠ TEST Ù„Ø§ ØªØ·Ø§Ø¨Ù‚ ALL!"

def count_images(root_dir):
    IMG_EXTS = {'.jpg','.jpeg','.png','.bmp','.webp'}
    data = []
    total = 0
    for cls in sorted(os.listdir(root_dir)):
        path = Path(root_dir)/cls
        if path.is_dir():
            n = sum(1 for f in path.iterdir() if f.is_file() and f.suffix.lower() in IMG_EXTS)
            data.append((cls, n))
            total += n
    return data, total

train_counts, train_total = count_images(ALL_DIR)
test_counts, test_total   = count_images(TEST_DIR)

df_train = pd.DataFrame(train_counts, columns=["Class", "Train Images"])
df_test  = pd.DataFrame(test_counts, columns=["Class", "Test Images"])

summary = pd.merge(df_train, df_test, on="Class", how="outer")
summary["Train %"] = (summary["Train Images"] / train_total * 100).round(2)
summary["Test %"]  = (summary["Test Images"]  / test_total  * 100).round(2)

print("\nðŸ“¦ Training Dataset (ALL):")
print("Total:", train_total)
print(df_train.to_string(index=False))

print("\nðŸ§ª Testing Dataset (TEST MIXED):")
print("Total:", test_total)
print(df_test.to_string(index=False))

print("\nðŸ“Š Summary Combined:")
print(summary.to_string(index=False))

def find_model_file(base_dir: str):
    candidates = []
    for root, dirs, files in os.walk(base_dir):
        for fn in files:
            if fn.endswith(".keras"):
                full = os.path.join(root, fn)
                priority = 0
                if fn == "final.keras": priority = 2
                elif fn == "best.keras": priority = 1
                candidates.append((priority, os.path.getmtime(full), full))
    if not candidates:
        return None
    candidates.sort(key=lambda t: (-t[0], -t[1]))
    return candidates[0][2]

model_path = None
for var_name in ["final_path", "ckpt_path"]:
    if var_name in globals():
        p = globals()[var_name]
        if isinstance(p, str) and os.path.exists(p):
            model_path = p; print(f">> Using existing path from memory: {var_name} = {p}")
            break

if model_path is None:
    model_path = find_model_file(BASE_DIR)
    if model_path:
        print(">> Found model file:", model_path)
    else:
        raise FileNotFoundError("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…ÙˆØ¯Ù„ Ù…Ø­ÙÙˆØ¸ ÙÙŠ Ø§Ù„Ø¯Ø±Ø§ÙŠÙ. Ø¯Ø±Ù‘Ø¨ÙŠ Ø£ÙˆÙ„Ø§Ù‹ Ø£Ùˆ Ø²ÙˆÙ‘Ø¯ÙŠ Ø§Ù„Ù…Ø³Ø§Ø± ÙŠØ¯ÙˆÙŠÙ‹Ø§.")

model = tf.keras.models.load_model(model_path)
print("âœ… Model loaded successfully!")

RUN_NAME   = f"finetune_boost_{time.strftime('%Y%m%d_%H%M%S')}"
OUTPUT_DIR = os.path.join(os.path.dirname(model_path), RUN_NAME)
os.makedirs(OUTPUT_DIR, exist_ok=True)
print("ðŸ“ OUTPUT_DIR:", OUTPUT_DIR)

with open(os.path.join(OUTPUT_DIR, "class_names.json"), "w", encoding="utf-8") as f:
    json.dump(class_names, f, ensure_ascii=False, indent=2)
print("âœ… class_names.json saved.")

!nvidia-smi -L || echo "No GPU"

import os, json, random, numpy as np, tensorflow as tf
from pathlib import Path
from collections import Counter

BASE_DIR  = "/content/drive/MyDrive/Colab Notebooks"
ALL_DIR   = f"{BASE_DIR}/all"
TEST_DIR  = f"{BASE_DIR}/TEST MIXED"

SEED        = 42
IMG_SIZE    = (224, 224)
BATCH_SIZE  = 16
VAL_SPLIT   = 0.15

from tensorflow.keras import mixed_precision
if not tf.config.list_physical_devices('GPU'):
    mixed_precision.set_global_policy("float32")
print("Policy:", mixed_precision.global_policy())

train_ds = tf.keras.utils.image_dataset_from_directory(
    ALL_DIR, validation_split=VAL_SPLIT, subset="training", seed=SEED,
    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=True
)
val_ds = tf.keras.utils.image_dataset_from_directory(
    ALL_DIR, validation_split=VAL_SPLIT, subset="validation", seed=SEED,
    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False
)

class_names = train_ds.class_names
num_classes = len(class_names)
print("Classes:", class_names)

test_ds = tf.keras.utils.image_dataset_from_directory(
    TEST_DIR, image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False
)

assert test_ds.class_names == class_names, f"âš ï¸ TEST classes mismatch:\n{test_ds.class_names} vs {class_names}"
print("âœ“ TEST classes match order")

AUTOTUNE = tf.data.AUTOTUNE
opt = tf.data.Options(); opt.experimental_deterministic = False
train_ds = train_ds.with_options(opt).prefetch(AUTOTUNE)
val_ds   = val_ds.prefetch(AUTOTUNE)
test_ds  = test_ds.prefetch(AUTOTUNE)

IMG_EXTS = {'.jpg','.jpeg','.png','.bmp','.webp'}
counts = {cls: 0 for cls in class_names}
for cls in class_names:
    p = Path(ALL_DIR) / cls
    if p.is_dir():
        counts[cls] = sum(1 for f in p.iterdir() if f.is_file() and f.suffix.lower() in IMG_EXTS)
print("Class counts (ALL):", counts)

counts_arr = np.array([counts[c] for c in class_names], dtype=np.float32)
inv_freq   = 1.0 / np.maximum(counts_arr, 1)
inv_freq   = inv_freq / inv_freq.mean()
alpha = 1.25
scaled = inv_freq ** alpha
class_weights = {i: float(scaled[i]) for i in range(num_classes)}
print("Tuned class_weights:", class_weights)

steps_per_epoch  = 160
validation_steps = None
print(f"steps_per_epoch={steps_per_epoch}, validation_steps={validation_steps}")

from tensorflow.keras import layers, models, callbacks

base = tf.keras.applications.MobileNetV2(include_top=False, weights="imagenet",
                                         input_shape=(*IMG_SIZE, 3))
base.trainable = False

inputs = layers.Input(shape=(*IMG_SIZE, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)
x = base(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.2)(x)
outputs = layers.Dense(num_classes, activation="softmax", dtype="float32")(x)
model = models.Model(inputs, outputs, name="MobileNetV2_fast")

model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

cbs = [
    callbacks.EarlyStopping(patience=2, restore_best_weights=True, monitor="val_accuracy"),
    callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor="val_loss", verbose=1),
]

print(">> Phase 1 (frozen) training...")
hist1 = model.fit(
    train_ds, validation_data=val_ds,
    epochs=4, class_weight=class_weights, callbacks=cbs, verbose=1,
    steps_per_epoch=steps_per_epoch, validation_steps=validation_steps
)

for l in base.layers: l.trainable = True
cut = int(len(base.layers) * 0.60)
for i, l in enumerate(base.layers):
    l.trainable = (i >= cut)
for l in base.layers:
    if isinstance(l, layers.BatchNormalization):
        l.trainable = False

model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

print(">> Phase 2 (fine-tune) training...")
hist2 = model.fit(
    train_ds, validation_data=val_ds,
    epochs=6, class_weight=class_weights, callbacks=cbs, verbose=1,
    steps_per_epoch=steps_per_epoch, validation_steps=validation_steps
)

val_loss, val_acc = model.evaluate(val_ds, verbose=1)
print(f"\n== Validation (full) ==  Top-1: {val_acc*100:.2f}%")

import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix from math import ceil

y_true = np.array([int(y.numpy()) for _, y in test_ds.unbatch()])
y_probs = model.predict(test_ds, verbose=1)
y_pred  = y_probs.argmax(axis=1)

top1 = (y_pred == y_true).mean()*100
top2 = (np.argsort(-y_probs, axis=1)[:, :2] == y_true[:, None]).any(axis=1).mean()*100
print(f"== TEST (full) ==  Top-1: {top1:.2f}%  |  Top-2: {top2:.2f}%")

cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(class_names)))
per_class_n       = cm.sum(axis=1)
per_class_correct = cm.diagonal()
per_class_acc     = per_class_correct / np.maximum(per_class_n, 1)
z = 1.96
se = np.sqrt(np.clip(per_class_acc*(1-per_class_acc)/np.maximum(per_class_n,1), 0, 1))
ci_low  = np.clip(per_class_acc - z*se, 0, 1)
ci_high = np.clip(per_class_acc + z*se, 0, 1)

df = pd.DataFrame({
    "Class": class_names,
    "N": per_class_n,
    "Acc %": (per_class_acc*100).round(2),
    "95% CI low %": (ci_low*100).round(2),
    "95% CI high %": (ci_high*100).round(2),
}).sort_values("Class").reset_index(drop=True)
print("\nPer-class accuracies with 95% CI:")
print(df.to_string(index=False))

def predict_tta(ds, model):
    outs = []
    for x,_ in ds:
        p1 = model.predict(x, verbose=0)
        p2 = model.predict(tf.image.flip_left_right(x), verbose=0)
        outs.append((p1+p2)/2.0)
    return np.vstack(outs)

y_probs_tta = predict_tta(test_ds, model)
tta_top1 = (y_probs_tta.argmax(axis=1) == y_true).mean()*100
print(f"TTA TEST Top-1: {tta_top1:.2f}%")

def to_uint8(img):
    x = np.array(img)
    return np.clip(x, 0, 255).astype(np.uint8)

N = 24
imgs, y_true_list = [], []
for bx, by in test_ds:
    for xi, yi in zip(bx, by):
        if len(imgs) < N:
            imgs.append(to_uint8(xi))
            y_true_list.append(int(yi.numpy()))
        else:
            break
    if len(imgs) >= N: break

probs_sample = model.predict(np.stack(imgs), verbose=0)
y_pred_sample = probs_sample.argmax(axis=1)
p_conf = probs_sample.max(axis=1)

COLS = 6
ROWS = ceil(N / COLS)
plt.figure(figsize=(COLS*3.1, ROWS*3.1))
for i in range(N):
    plt.subplot(ROWS, COLS, i+1)
    plt.imshow(imgs[i]); plt.axis('off')
    true_name = class_names[y_true_list[i]]
    pred_name = class_names[y_pred_sample[i]]
    conf = p_conf[i]*100.0
    plt.title(f"True: {true_name}", color="black", fontsize=9, pad=2)
    plt.xlabel(f"Pred: {pred_name} ({conf:.1f}%)",
               color=("green" if y_pred_sample[i]==y_true_list[i] else "red"),
               fontsize=9, labelpad=2)
    ax = plt.gca()
    for s in ax.spines.values():
        s.set_edgecolor("green" if y_pred_sample[i]==y_true_list[i] else "red")
        s.set_linewidth(2.0)

plt.suptitle("Visual check â€” ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ÙˆØ¯Ù„ Ø¨Ø§Ù„ØµÙˆØ± (Ø£Ø®Ø¶Ø±=ØµØ­ / Ø£Ø­Ù…Ø±=ØºÙ„Ø·)", fontsize=12, y=1.02)
plt.tight_layout()
plt.show()

idx_wrong = np.where(y_pred_sample != np.array(y_true_list)[:len(y_pred_sample)])[0]
if idx_wrong.size > 0:
    conf_wrong = p_conf[idx_wrong]
    top_bad = idx_wrong[np.argsort(-conf_wrong)[:12]]
    print("\nØ£Ø³ÙˆØ£ 12 Ø®Ø·Ø£ Ø¨Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© (Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ© Ø£Ø¹Ù„Ø§Ù‡):", top_bad.tolist())
else:
    print("\nÙ…Ø§ÙÙŠ Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¹ÙŠÙ‘Ù†Ø© ðŸ‘")

import os, time, numpy as np, pandas as pd, matplotlib.pyplot as plt, tensorflow as tf
from tensorflow.keras import layers, callbacks
from sklearn.metrics import confusion_matrix
from math import ceil
from pathlib import Path

assert 'model' in globals() and 'train_ds' in globals() and 'val_ds' in globals() and 'test_ds' in globals() and 'class_names' in globals(), \
    "Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ´ØºÙŠÙ„ Ø®Ù„Ø§ÙŠØ§ (1) Ùˆ (2) Ø£ÙˆÙ„Ø§Ù‹."

for lyr in model.layers:
    lyr.trainable = True
    if isinstance(lyr, layers.BatchNormalization):
        lyr.trainable = False

PSORIASIS_CLASS_INDEX = 4
BOOST_FACTOR = 2.5

class_weights = {i: 1.0 for i in range(len(class_names))}
class_weights[PSORIASIS_CLASS_INDEX] = BOOST_FACTOR
print("class_weights (Psoriasis Boosted):", class_weights)

model.compile(
    optimizer=tf.keras.optimizers.Adam(5e-6),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

steps_per_epoch  = 160
validation_steps = None
cbs = [
    callbacks.EarlyStopping(patience=1, restore_best_weights=True, monitor="val_accuracy"),
    callbacks.ReduceLROnPlateau(patience=0, factor=0.5, monitor="val_loss", verbose=1),
]

print(">> Fine-tuning boost FOCUSED on Psoriasis is running...")
hist = model.fit(
    train_ds, validation_data=val_ds,
    epochs=4, class_weight=class_weights,
    steps_per_epoch=steps_per_epoch, validation_steps=validation_steps,
    callbacks=cbs, verbose=1
)
print("âœ“ Psoriasis Focus Fine-tune done.")

val_loss, val_acc = model.evaluate(val_ds, verbose=1)
print(f"\n== Validation (full) ==  Top-1: {val_acc*100:.2f}%")

y_true = np.array([int(y.numpy()) for _, y in test_ds.unbatch()])
y_probs = model.predict(test_ds, verbose=1)
y_pred  = y_probs.argmax(axis=1)
top1 = (y_pred == y_true).mean()*100
top2 = (np.argsort(-y_probs, axis=1)[:, :2] == y_true[:, None]).any(axis=1).mean()*100
print(f"== TEST (full) AFTER PSORIASIS BOOST ==  Top-1: {top1:.2f}%  |  Top-2: {top2:.2f}%")

cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(class_names)))
per_class_n      = cm.sum(axis=1)
per_class_correct = cm.diagonal()
per_class_acc    = per_class_correct / np.maximum(per_class_n, 1.0)
z = 1.96
se = np.sqrt(np.clip(per_class_acc*(1-per_class_acc)/np.maximum(per_class_n,1), 0, 1))
ci_low, ci_high = np.clip(per_class_acc - z*se, 0, 1), np.clip(per_class_acc + z*se, 0, 1)

df = pd.DataFrame({
    "Class": class_names,
    "N": per_class_n,
    "Acc %": (per_class_acc*100).round(2),
    "95% CI low %": (ci_low*100).round(2),
    "95% CI high %": (ci_high*100).round(2),
}).sort_values("Class").reset_index(drop=True)

print("\nPer-class accuracies with 95% CI (AFTER PSORIASIS BOOST):")
print(df.to_string(index=False))

def predict_tta(ds, model):
    outs = []
    for x,_ in ds:
        p1 = model.predict(x, verbose=0)
        p2 = model.predict(tf.image.flip_left_right(x), verbose=0)
        outs.append((p1+p2)/2.0)
    return np.vstack(outs)

y_probs_tta = predict_tta(test_ds, model)
tta_top1 = (y_probs_tta.argmax(axis=1) == y_true).mean()*100
print(f"\nTTA TEST Top-1: {tta_top1:.2f}%")

OUTPUT_DIR_FOCUSED = os.path.join(os.path.dirname(os.path.dirname(model_path)), "finetune_boost_focused_psoriasis")
os.makedirs(OUTPUT_DIR_FOCUSED, exist_ok=True)
final_path_focused = os.path.join(OUTPUT_DIR_FOCUSED, "final_psoriasis_boost.keras")
model.save(final_path_focused)
print("âœ“ Saved new focused model:", final_path_focused)

df.to_csv(os.path.join(OUTPUT_DIR_FOCUSED, "per_class_acc_psoriasis_boost.csv"), index=False)
print("âœ“ Saved new per-class accuracy table.")

# =========================
# Progressive Fine-Tune (224x224) â€” No Augmentation
# AdamW + Cosine Warmup + EMA + BN Frozen
# =========================

from google.colab import drive
drive.mount('/content/drive', force_remount=False)

import os, json, numpy as np, tensorflow as tf
from tensorflow.keras import layers, callbacks

SEED        = 42
IMG_SIZE    = (224, 224)
BATCH_SIZE  = 16
VAL_SPLIT   = 0.15
BASE_DIR    = "/content/drive/MyDrive/Colab Notebooks"
ALL_DIR     = f"{BASE_DIR}/all"
EXPLICIT_MODEL_PATH = None

USE_CLASS_WEIGHTS = True

print(tf.__version__)
print("TF devices (GPU?):", tf.config.list_physical_devices())

def find_model_file(base_dir: str):
    """Ø§Ø¨Ø­Ø«ÙŠ Ø¹Ù† final.keras Ø«Ù… best.keras Ø«Ù… Ø£Ø­Ø¯Ø« .keras"""
    candidates = []
    for root, dirs, files in os.walk(base_dir):
        for fn in files:
            if fn.endswith(".keras"):
                full = os.path.join(root, fn)
                prio = 0
                if fn == "final.keras": prio = 3
                elif fn == "final_noaug.keras": prio = 2
                elif fn == "best.keras": prio = 1
                candidates.append((prio, os.path.getmtime(full), full))
    if not candidates: return None
    candidates.sort(key=lambda t: (-t[0], -t[1]))
    return candidates[0][2]

def preprocess_eval(x, y):
    x = tf.image.resize(x, IMG_SIZE)
    x = tf.keras.applications.mobilenet_v2.preprocess_input(x)
    return x, y

def freeze_batchnorm(m):
    """Ø«Ø¨Ù‘Øª ÙƒÙ„ Ø·Ø¨Ù‚Ø§Øª BatchNorm (Ø­ØªÙ‰ Ù„Ùˆ Ø¯Ø§Ø®Ù„ Ù†Ù…Ø§Ø°Ø¬ ÙØ±Ø¹ÙŠØ©)"""
    for l in m.layers:
        if isinstance(l, layers.BatchNormalization):
            l.trainable = False
        if hasattr(l, "layers"):
            freeze_batchnorm(l)

def set_trainable_portion(base_model, portion: float):
    """
    portion=0.40 ÙŠØ¹Ù†ÙŠ ÙØªØ­ Ø¢Ø®Ø± 40% Ù…Ù† Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù€base ÙÙ‚Ø·.
    BatchNorm ØªØ¨Ù‚Ù‰ Ø«Ø§Ø¨ØªØ© Ø¯ÙˆÙ…Ù‹Ø§.
    """
    all_layers = list(base_model.layers)
    cut = int(len(all_layers) * (1.0 - portion))
    for i, l in enumerate(all_layers):
        l.trainable = (i >= cut)
    freeze_batchnorm(base_model)

def get_steps(ds):
    card = tf.data.experimental.cardinality(ds).numpy()
    return 160 if card < 0 else int(card)

def make_warmcosine(total_steps, base_lr, min_lr):
    """Cosine LR + 5% Warmup â€” Ø¢Ù…Ù† Ù„ÙƒÙ„ Ø¥ØµØ¯Ø§Ø±Ø§Øª Keras (ÙŠØ³ØªØ¹Ù…Ù„ .learning_rate Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª)"""
    warmup_steps = max(1, int(0.05 * total_steps))

    def lr_schedule(step):
        step = float(step)
        if step < warmup_steps:
            return base_lr * (step / max(1.0, float(warmup_steps)))
        cos_steps = max(1.0, float(total_steps - warmup_steps))
        return min_lr + 0.5 * (base_lr - min_lr) * (1.0 + np.cos(np.pi * (step - warmup_steps) / cos_steps))

    class WarmCosine(callbacks.Callback):
        def _set_lr(self, value):
            opt = self.model.optimizer
            if hasattr(opt, "learning_rate"):
                try:
                    opt.learning_rate.assign(value)
                except Exception:
                    tf.keras.backend.set_value(opt.learning_rate, value)
            elif hasattr(opt, "lr"):
                try:
                    opt.lr.assign(value)
                except Exception:
                    tf.keras.backend.set_value(opt.lr, value)
            else:
                for var in opt.variables():
                    if "learning_rate" in var.name or var.name.endswith("/lr:0"):
                        tf.keras.backend.set_value(var, value)
                        break

        def on_train_batch_begin(self, batch, logs=None):
            step = int(self.model.optimizer.iterations.numpy())
            self._set_lr(lr_schedule(step))

    return WarmCosine()

class EMA(callbacks.Callback):
    """Exponential Moving Average Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆØ²Ø§Ù† â€” ÙŠØ­ÙØ¸ Ù„Ù‚Ø·Ø© EMA Ø¨Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„ÙƒÙ„ Ù…Ø±Ø­Ù„Ø©"""
    def __init__(self, decay=0.999, ema_path="final_ema.keras"):
        super().__init__()
        self.decay = decay
        self.ema_weights = None
        self.ema_path = ema_path
    def on_train_begin(self, logs=None):
        self.ema_weights = [w.numpy() for w in self.model.weights]
    def on_batch_end(self, batch, logs=None):
        for i, w in enumerate(self.model.weights):
            self.ema_weights[i] = self.decay * self.ema_weights[i] + (1.0 - self.decay) * w.numpy()
    def on_train_end(self, logs=None):
        orig = self.model.get_weights()
        self.model.set_weights(self.ema_weights)
        self.model.save(self.ema_path)
        print(f"âœ“ Saved EMA model: {self.ema_path}")
        self.model.set_weights(orig)

def make_optimizer(base_lr, weight_decay=1e-4):
    """ÙŠÙØ¶Ù‘Ù„ AdamWØ› Ø¥Ù† Ù„Ù… ÙŠØªÙˆÙØ± ÙŠØ³Ù‚Ø· Ø¹Ù„Ù‰ Adam"""
    for path in ["tf.keras.optimizers.AdamW","tf.keras.optimizers.experimental.AdamW"]:
        try:
            AdamW = eval(path)
            print(f"âœ“ Using {path}")
            return AdamW(learning_rate=base_lr, weight_decay=weight_decay)
        except Exception:
            pass
    print("âš ï¸ AdamW ØºÙŠØ± Ù…ØªÙˆÙØ±Ø› Ø£Ø³ØªØ®Ø¯Ù… Adam (Ø¨Ø¯ÙˆÙ† weight decay).")
    return tf.keras.optimizers.Adam(learning_rate=base_lr)

from tensorflow.keras.preprocessing import image_dataset_from_directory

train_raw = image_dataset_from_directory(
    ALL_DIR, validation_split=VAL_SPLIT, subset="training", seed=SEED,
    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=True)

val_raw = image_dataset_from_directory(
    ALL_DIR, validation_split=VAL_SPLIT, subset="validation", seed=SEED,
    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False)

class_names = train_raw.class_names
num_classes = len(class_names)
print("Classes:", class_names)

train_ds = (train_raw.map(preprocess_eval, num_parallel_calls=tf.data.AUTOTUNE)
                     .cache().shuffle(4096).prefetch(tf.data.AUTOTUNE))
val_ds   = (val_raw.map(preprocess_eval, num_parallel_calls=tf.data.AUTOTUNE)
                   .cache().prefetch(tf.data.AUTOTUNE))

class_weights = None
if USE_CLASS_WEIGHTS:
    import pathlib
    IMG_EXTS = {'.jpg','.jpeg','.png','.bmp','.webp'}
    counts = []
    for cls in class_names:
        p = pathlib.Path(ALL_DIR)/cls
        n = sum(1 for f in p.iterdir() if f.is_file() and f.suffix.lower() in IMG_EXTS)
        counts.append(max(n,1))
    counts = np.array(counts, dtype=np.float32)
    inv = 1.0 / counts
    inv = inv / inv.mean()
    alpha = 1.25
    scaled = inv ** alpha
    class_weights = {i: float(scaled[i]) for i in range(num_classes)}
    print("Class weights:", class_weights)

if EXPLICIT_MODEL_PATH and os.path.exists(EXPLICIT_MODEL_PATH):
    model_path = EXPLICIT_MODEL_PATH
else:
    model_path = find_model_file(BASE_DIR)
assert model_path is not None, "âŒ Ù…Ø§ ÙˆØ¬Ø¯Øª Ø£ÙŠ Ù…Ù„Ù .keras â€” Ø­Ø¯Ø¯ÙŠ EXPLICIT_MODEL_PATH Ø£Ùˆ Ø¯Ø±Ù‘Ø¨ÙŠ/Ø§Ø­ÙØ¸ÙŠ Ù…ÙˆØ¯Ù„ Ø£ÙˆÙ„Ø§Ù‹."
print(">> Using model:", model_path)

model = tf.keras.models.load_model(model_path)
print("Model loaded.")

def find_conv_base(m):
    for l in m.layers:
        if isinstance(l, tf.keras.Model) and ("mobilenet" in l.name.lower() or "MobileNet" in l.__class__.__name__):
            return l
    return m.layers[1] if isinstance(m.layers[1], tf.keras.Model) else m

base = find_conv_base(model)
print("Conv base layer:", base.name if hasattr(base,'name') else type(base))

stages = [
    (0.40, 6,  1.5e-5, 1.0e-6, "p40"),
    (0.70, 8,  1.0e-5, 5.0e-7, "p70"),
    (1.00, 10, 7.5e-6, 3.0e-7, "p100"),
]

loss = tf.keras.losses.SparseCategoricalCrossentropy()
steps_per_epoch = get_steps(train_ds)

best_ckpts = []
ema_snaps  = []

for portion, EPOCHS, BASE_LR, MIN_LR, tag in stages:
    print(f"\n====== Stage {tag}: unfreeze last {int(portion*100)}% | epochs={EPOCHS} | lr~[{MIN_LR}, {BASE_LR}] ======")
    for l in model.layers: l.trainable = True
    set_trainable_portion(base, portion)

    opt = make_optimizer(BASE_LR, weight_decay=1e-4)
    model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])

    total_steps = max(1, steps_per_epoch) * EPOCHS
    warmcos = make_warmcosine(total_steps, BASE_LR, MIN_LR)
    ckpt_path = f"best_{tag}.keras"
    ema_cb = EMA(decay=0.999, ema_path=f"final_ema_{tag}.keras")

    cbs = [
        warmcos,
        callbacks.ModelCheckpoint(ckpt_path, monitor="val_accuracy", save_best_only=True, verbose=1),
        callbacks.EarlyStopping(patience=3, restore_best_weights=True, monitor="val_accuracy"),
        callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor="val_loss", verbose=1),
        ema_cb,
    ]

    try:
        model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS,
                  callbacks=cbs,
                  class_weight=class_weights if class_weights is not None else None,
                  verbose=1)
    except TypeError:
        if class_weights is None:
            model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS,
                      callbacks=cbs, verbose=1)
        else:
            model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS,
                      callbacks=cbs, class_weight=class_weights, verbose=1)

    best_ckpts.append(ckpt_path)
    ema_snaps.append(ema_cb.ema_path)

model.save("final_progressive_ft_224.keras")
print("\nâœ“ Saved final model: final_progressive_ft_224.keras")
print("Best checkpoints:", best_ckpts)
print("EMA snapshots:",  ema_snaps)

val_loss, val_acc = model.evaluate(val_ds, verbose=1)
print(f"== VAL (no TTA) Top-1: {val_acc*100:.2f}%")

def predict_tta(ds, model):
    outs = []
    for x,_ in ds:
        p1 = model.predict(x, verbose=0)
        p2 = model.predict(tf.image.flip_left_right(x), verbose=0)
        outs.append((p1+p2)/2.0)
    return np.vstack(outs)

print("== VAL with TTA (horizontal flip only)â€¦")
probs_val = predict_tta(val_ds, model)
y_val = np.array([int(y.numpy()) for _, y in val_ds.unbatch()])
acc_tta = (np.argmax(probs_val, axis=1) == y_val).mean()*100
print(f"VAL Top-1 (TTA): {acc_tta:.2f}%")

import io, numpy as np, pandas as pd, tensorflow as tf
from PIL import Image
from google.colab import files

try:
    IMG_SIZE
except NameError:
    IMG_SIZE = (224, 224)

def preprocess_img(img_pil, target_size=IMG_SIZE):
    img = img_pil.convert("RGB").resize(target_size)
    x = np.array(img).astype("float32")
    x = tf.keras.applications.mobilenet_v2.preprocess_input(x)
    return np.expand_dims(x, 0), img

assert 'model' in globals(), "âŒ Ø§Ù„Ù…ÙˆØ¯Ù„ ØºÙŠØ± Ù…Ø¹Ø±Ù‘Ù (model). Ø­Ù…Ù‘Ù„ÙŠÙ‡ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø± Ø£Ùˆ Ø´ØºÙ‘Ù„ÙŠ Ø§Ù„Ø®Ù„Ø§ÙŠØ§ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©."
assert 'class_names' in globals(), "âŒ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª ØºÙŠØ± Ù…Ø¹Ø±Ù‘ÙØ© (class_names). Ø´ØºÙ‘Ù„ÙŠ Ø§Ù„Ø®Ù„Ø§ÙŠØ§ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©."

uploaded = files.upload()

for fname, content in uploaded.items():
    img_pil = Image.open(io.BytesIO(content))
    x, img_disp = preprocess_img(img_pil, IMG_SIZE)

    probs = model.predict(x, verbose=0)[0]

    pred_idx = int(np.argmax(probs))
    pred_name = class_names[pred_idx]
    pred_conf = float(probs[pred_idx] * 100)

    print(f"\nðŸ“Œ Ø§Ù„Ù…Ù„Ù: {fname}")
    print(f"âœ… Ø§Ù„Ù…Ø±Ø¶ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {pred_name}  |  Ø§Ù„Ø«Ù‚Ø©: {pred_conf:.2f}%")

    df = pd.DataFrame({
        "Disease": class_names,
        "Confidence %": (probs * 100).round(2)
    }).sort_values("Confidence %", ascending=False).reset_index(drop=True)

    df.insert(0, "Top", ["âž¡ï¸" if i == 0 else "" for i in range(len(df))])

    from IPython.display import display
    display(df)

    out_csv = fname.rsplit(".", 1)[0] + "_probs.csv"
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"ðŸ’¾ ØªÙ… Ø­ÙØ¸ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª: {out_csv}")

    try:
        import matplotlib.pyplot as plt
        plt.figure(figsize=(4,4))
        plt.imshow(img_disp)
        plt.axis('off')
        plt.title(f"{pred_name} â€” {pred_conf:.1f}%")
        plt.show()
    except Exception as e:
        print("Ù…Ù„Ø§Ø­Ø¸Ø©: Ù„Ù… ÙŠØªÙ… Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© (matplotlib ØºÙŠØ± Ù…ØªÙˆÙØ±).", e)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import tensorflow as tf

y_true = np.array([y.numpy() for _, y in test_ds.unbatch()])

y_probs = model.predict(test_ds, verbose=1)
y_pred = np.argmax(y_probs, axis=1)

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(8,6))
plt.imshow(cm, cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.xticks(np.arange(len(class_names)), class_names, rotation=45)
plt.yticks(np.arange(len(class_names)), class_names)

thresh = cm.max() / 2
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j],
                 ha='center', va='center',
                 color='white' if cm[i, j] > thresh else 'black')

plt.colorbar()
plt.tight_layout()
plt.show()

import tensorflow as tf
import numpy as np

train_loss, train_acc = model.evaluate(train_ds, verbose=1)
print(f"ðŸŽ¯ TRAIN Top-1 Accuracy: {train_acc*100:.2f}%  |  Loss: {train_loss:.4f}")

try:
    val_loss, val_acc = model.evaluate(val_ds, verbose=1)
    print(f"ðŸ§ª VAL Top-1 Accuracy:   {val_acc*100:.2f}%  |  Loss: {val_loss:.4f}")
except NameError:
    pass